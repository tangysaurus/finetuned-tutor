{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install torch transformers datasets accelerate peft trl\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "TYyk82uKOLFk",
        "collapsed": true
      },
      "id": "TYyk82uKOLFk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocess MathDial dataset for finetuning"
      ],
      "metadata": {
        "id": "taQrJ8hOcR04"
      },
      "id": "taQrJ8hOcR04"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ff724d55",
      "metadata": {
        "id": "ff724d55"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from datasets import load_dataset, Dataset\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "import re\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7e519d00",
      "metadata": {
        "id": "7e519d00"
      },
      "outputs": [],
      "source": [
        "HUGGING_FACE_ACCESS_TOKEN = userdata.get('HUGGING_FACE_ACCESS_TOKEN')\n",
        "login(token = HUGGING_FACE_ACCESS_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58cc5c38",
      "metadata": {
        "id": "58cc5c38"
      },
      "outputs": [],
      "source": [
        "# load MathDial dataset\n",
        "tutor_dialogue = load_dataset(\"eth-nlped/mathdial\")\n",
        "tutor_train = tutor_dialogue[\"train\"].to_pandas()\n",
        "tutor_test = tutor_dialogue[\"test\"].to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "13ffc28a",
      "metadata": {
        "id": "13ffc28a"
      },
      "outputs": [],
      "source": [
        "def extract_move(sentence: str):\n",
        "    \"\"\"extract teacher moves (probing, focus, telling, generic)\"\"\"\n",
        "    pattern = r\"\\b(probing|focus|telling|generic)\\b\"\n",
        "    match = re.search(rf\"\\({pattern}\\)\", sentence)\n",
        "\n",
        "    if match:\n",
        "        clean_sentence = re.sub(rf\"\\({pattern}\\)\", \"\", sentence)\n",
        "        return clean_sentence, match.group(1)\n",
        "\n",
        "    else:\n",
        "        return sentence, None\n",
        "\n",
        "def remove_names(sentence: str, student_name: str):\n",
        "    \"\"\"remove names from conversation\"\"\"\n",
        "    pattern = rf\"\\b(Teacher|Student|{student_name})\\b\"\n",
        "    match = re.search(rf\"{pattern}: \", sentence)\n",
        "\n",
        "    if match:\n",
        "        clean_sentence = re.sub(rf\"{pattern}: \", \"\", sentence)\n",
        "        return clean_sentence, match.group(1)\n",
        "\n",
        "    else:\n",
        "        return sentence, None\n",
        "\n",
        "def preprocess(conversation: str):\n",
        "    \"\"\"split conversation into turns\"\"\"\n",
        "\n",
        "    moves = []\n",
        "    split_conversation = conversation.split(\"|EOM|\")\n",
        "\n",
        "    for i in range(len(split_conversation)):\n",
        "        clean_sentence, move = extract_move(split_conversation[i])\n",
        "        split_conversation[i] = clean_sentence\n",
        "\n",
        "        if move:\n",
        "            moves.append(move)\n",
        "\n",
        "    split_conversation = list(filter(None, split_conversation))\n",
        "    return split_conversation, moves\n",
        "\n",
        "def format_conversation(\n",
        "        split_conversation: list[str],\n",
        "        moves: list[str],\n",
        "        system_prompt: str,\n",
        "        incorrect_solution: str,\n",
        "        student_name: str\n",
        "    ):\n",
        "    \"\"\"\n",
        "    formats conversation into the following structure:\n",
        "    '{\"messages\": [{\"role\": \"user\", \"content\": \"What color is the sky?\"}, {\"role\": \"assistant\", \"content\": \"It is blue.\"}]}'\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": incorrect_solution}\n",
        "                ]\n",
        "\n",
        "    move_index = 0\n",
        "\n",
        "    for i in range(len(split_conversation)):\n",
        "        clean_sentence, match = remove_names(split_conversation[i], student_name)\n",
        "        if not clean_sentence or not match:\n",
        "            continue\n",
        "\n",
        "        if match == \"Teacher\":\n",
        "            messages.append({\"role\": \"assistant\", \"content\": clean_sentence})\n",
        "            move_index += 1\n",
        "\n",
        "        else:\n",
        "            messages.append({\"role\": \"user\", \"content\": clean_sentence})\n",
        "\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "d8856618",
      "metadata": {
        "id": "d8856618"
      },
      "outputs": [],
      "source": [
        "def build_system_prompt(question: str, ground_truth: str, student_name):\n",
        "    system_prompt = (\n",
        "        f\"\"\"\n",
        "        You are a math tutor. Your student {student_name} is trying to solve the following problem: {question}.\n",
        "        Given the correct solution, your goal is to help them solve the problem by guiding them with questions and hints.\n",
        "        Here is the correct solution: {ground_truth}\n",
        "        \"\"\"\n",
        "    )\n",
        "    return system_prompt\n",
        "\n",
        "def get_student_name(student_profile: str):\n",
        "    return student_profile.split()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "d745ff08",
      "metadata": {
        "id": "d745ff08"
      },
      "outputs": [],
      "source": [
        "def process_row(row):\n",
        "    \"\"\"Constructs a string of messages for each conversation\"\"\"\n",
        "    student_name = get_student_name(row[\"student_profile\"])\n",
        "    system_prompt = build_system_prompt(row[\"question\"], row[\"ground_truth\"], student_name)\n",
        "    split_conversation = preprocess(row[\"conversation\"])\n",
        "    messages = format_conversation(split_conversation[0], split_conversation[1], system_prompt, row[\"student_incorrect_solution\"], student_name)\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "4e8218c9",
      "metadata": {
        "id": "4e8218c9"
      },
      "outputs": [],
      "source": [
        "tutor_train[\"messages\"] = tutor_train.apply(process_row, axis = 1)\n",
        "tutor_test[\"messages\"] = tutor_test.apply(process_row, axis = 1)\n",
        "\n",
        "train = Dataset.from_pandas(tutor_train[[\"messages\"]])\n",
        "test = Dataset.from_pandas(tutor_test[[\"messages\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Finetune using LoRA"
      ],
      "metadata": {
        "id": "g-G3MiCnc5Ek"
      },
      "id": "g-G3MiCnc5Ek"
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "2ftwdOwMOEPe"
      },
      "id": "2ftwdOwMOEPe",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Hoc5CH4wIo2A",
        "outputId": "f0bca10d-a4e4-449e-f419-4b2c9c082672",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Hoc5CH4wIo2A",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"HuggingFaceTB/SmolLM3-3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "sJmGanSRQxMQ"
      },
      "id": "sJmGanSRQxMQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank_dimension = 8\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.05\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r = rank_dimension,\n",
        "    lora_alpha = lora_alpha,\n",
        "    lora_dropout = lora_dropout,\n",
        "    bias = \"none\",\n",
        "    target_modules = [\"q_proj\", \"v_proj\"],\n",
        "    task_type = \"CAUSAL_LM\"\n",
        ")"
      ],
      "metadata": {
        "id": "RGd7g8zzPrpm"
      },
      "id": "RGd7g8zzPrpm",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(example):\n",
        "    return [tokenizer.apply_chat_template(\n",
        "        example[\"messages\"], tokenize=False, add_generation_prompt=False\n",
        "    )]"
      ],
      "metadata": {
        "id": "gfeI9ncmZzp1"
      },
      "id": "gfeI9ncmZzp1",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sft_config = SFTConfig(\n",
        "    output_dir = \"./results\",\n",
        "    logging_dir = \"./logs\",\n",
        "    num_train_epochs = 3,\n",
        "    per_device_train_batch_size = 8,\n",
        "    learning_rate = 2e-5,\n",
        "    warmup_ratio = 0.03,\n",
        "    weight_decay = 0.01,\n",
        "    save_strategy = \"steps\",\n",
        "    logging_steps = 10,\n",
        "    eval_strategy = \"steps\",\n",
        "    eval_steps = 200,\n",
        "    save_steps = 200,\n",
        "    max_length = 1024,\n",
        "    report_to=\"tensorboard\",\n",
        "    fp16 = True,\n",
        "    assistant_only_loss = True\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = train,\n",
        "    eval_dataset = test,\n",
        "    args = sft_config,\n",
        "    peft_config = peft_config,\n",
        "    formatting_func = formatting_func\n",
        ")"
      ],
      "metadata": {
        "id": "S11BRbofQNPk"
      },
      "id": "S11BRbofQNPk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model(\"./results/final_model\")"
      ],
      "metadata": {
        "id": "xECylEXGaKyA"
      },
      "id": "xECylEXGaKyA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "YJEAw-Vwboie"
      },
      "id": "YJEAw-Vwboie",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Perform inference using new adapter weights"
      ],
      "metadata": {
        "id": "JtjhTZwwxnXZ"
      },
      "id": "JtjhTZwwxnXZ"
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "5-R7th0fFM17"
      },
      "id": "5-R7th0fFM17",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"HuggingFaceTB/SmolLM3-3B\"\n",
        "adapter_path = \"results/final_model\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# tokenizer.pad_token = \"<pad>\"\n",
        "base = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "model = PeftModel.from_pretrained(base, adapter_path)\n",
        "\n",
        "model = model.to(\"cuda\")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "i2LP44oNxFYp"
      },
      "id": "i2LP44oNxFYp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(example):\n",
        "    return tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )"
      ],
      "metadata": {
        "id": "IYGi_mdNiOex"
      },
      "id": "IYGi_mdNiOex",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_turn(conversation: dict, user_input: str = None):\n",
        "    # Add user input to conversation\n",
        "    if user_input:\n",
        "        conversation[\"messages\"].append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Format with chat template, ensuring assistant is next\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        conversation[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True   # tells model: \"assistant should speak next\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate assistant reply\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=128,\n",
        "            do_sample=True,             # sampling helps avoid repetition\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "    # Slice out the new tokens\n",
        "    gen_tokens = outputs[:, inputs[\"input_ids\"].shape[1]:]\n",
        "    response = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "    # Append assistant reply\n",
        "    conversation[\"messages\"].append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "    print(\"\\nAssistant:\", response, \"\\n\")\n",
        "    return conversation\n"
      ],
      "metadata": {
        "id": "Rr1TBucoOnHj"
      },
      "id": "Rr1TBucoOnHj",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"A bakery sells muffins in boxes of 6. Each box costs $9. If Dayton has $45, how many muffins can he buy?\"\n",
        "ground_truth = \"First divide the total amount of money Dayton has by the cost of each box of muffins. This is the number of boxes of muffins Dayton can buy. The result will be $45/$9 = 5. Then multiply the number of boxes Dayton can buy by the number of muffins in each box. The result will be 5 * 6 = 30.\"\n",
        "system_prompt = build_system_prompt(question, ground_truth, \"Mia\")\n",
        "conversation = {\"messages\": [{\"role\": \"system\", \"content\": system_prompt}]}"
      ],
      "metadata": {
        "id": "IkO4knlnFt2M"
      },
      "id": "IkO4knlnFt2M",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"student: \")\n",
        "    if user_input == \"q\":\n",
        "        break\n",
        "    conversation = chat_turn(conversation, user_input)"
      ],
      "metadata": {
        "id": "Efa3lIITQSe6"
      },
      "id": "Efa3lIITQSe6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
